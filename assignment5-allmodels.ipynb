{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":68479,"databundleVersionId":10950255,"sourceType":"competition"}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Load the datasets\ntrain = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\n\n# Encode categorical variables\nle = LabelEncoder()\ntrain['NObeyesdad'] = le.fit_transform(train['NObeyesdad'])\nclasses = le.classes_  # Save class labels\nfor col in ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS']:\n    train[col] = le.fit_transform(train[col])\n    test[col] = test[col].map(lambda x: le.transform([x])[0] if x in le.classes_ else -1)\n\n# Feature selection\nX = train.drop(columns=['id', 'NObeyesdad'])\ny = train['NObeyesdad']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1234)\n\n# Standardize numerical features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\ntest_data = scaler.transform(test.drop(columns=['id']))\n\n# Model training & evaluation\ndef train_evaluate_model(model, param_grid, name):\n    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    best_model = grid_search.best_estimator_\n    y_pred = best_model.predict(X_val)\n    acc = accuracy_score(y_val, y_pred)\n    print(f'{name} Best Accuracy: {acc:.4f}')\n    print(classification_report(y_val, y_pred))\n    return best_model\n\n# Train models with hyperparameter tuning\nlog_reg = train_evaluate_model(LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=500), {'C': [0.1, 1, 10]}, 'Logistic Regression')\nlda = train_evaluate_model(LinearDiscriminantAnalysis(), {'solver': ['svd', 'lsqr']}, 'LDA')\nqda = train_evaluate_model(QuadraticDiscriminantAnalysis(), {'reg_param': [0.0, 0.1, 0.5]}, 'QDA')\nnb = train_evaluate_model(GaussianNB(), {'var_smoothing': [1e-9, 1e-8, 1e-7]}, 'Naive Bayes')\nsvm = train_evaluate_model(SVC(kernel='linear', probability=True), {'C': [0.1, 1, 10]}, 'SVM')\n\n# Generate test predictions and submission files\nmodels = {'Logistic_Regression': log_reg, 'LDA': lda, 'QDA': qda, 'Naive_Bayes': nb, 'SVM': svm}\nfor name, model in models.items():\n    predictions = np.vectorize(lambda x: classes[x] if x < len(classes) else 'Unknown')(model.predict(test_data))\n    submission = pd.DataFrame({'id': test['id'], 'NObeyesdad': predictions})\n    submission.to_csv(f'submission_{name}.csv', index=False)\n    print(f'Submission file for {name} saved.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T23:34:59.858604Z","iopub.execute_input":"2025-03-23T23:34:59.858975Z","iopub.status.idle":"2025-03-23T23:38:44.038681Z","shell.execute_reply.started":"2025-03-23T23:34:59.858947Z","shell.execute_reply":"2025-03-23T23:38:44.037524Z"}},"outputs":[{"name":"stdout","text":"Logistic Regression Best Accuracy: 0.8687\n              precision    recall  f1-score   support\n\n           0       0.88      0.95      0.91       510\n           1       0.86      0.82      0.84       579\n           2       0.84      0.84      0.84       588\n           3       0.94      0.96      0.95       649\n           4       1.00      1.00      1.00       830\n           5       0.75      0.69      0.72       491\n           6       0.71      0.72      0.72       505\n\n    accuracy                           0.87      4152\n   macro avg       0.85      0.85      0.85      4152\nweighted avg       0.87      0.87      0.87      4152\n\nLDA Best Accuracy: 0.8148\n              precision    recall  f1-score   support\n\n           0       0.83      0.90      0.86       510\n           1       0.76      0.70      0.73       579\n           2       0.79      0.76      0.77       588\n           3       0.91      0.96      0.93       649\n           4       0.99      0.99      0.99       830\n           5       0.64      0.64      0.64       491\n           6       0.65      0.63      0.64       505\n\n    accuracy                           0.81      4152\n   macro avg       0.79      0.80      0.79      4152\nweighted avg       0.81      0.81      0.81      4152\n\nQDA Best Accuracy: 0.7281\n              precision    recall  f1-score   support\n\n           0       0.80      0.93      0.86       510\n           1       0.69      0.58      0.63       579\n           2       0.53      0.72      0.61       588\n           3       0.80      0.96      0.87       649\n           4       0.98      0.99      0.99       830\n           5       0.59      0.30      0.40       491\n           6       0.49      0.38      0.43       505\n\n    accuracy                           0.73      4152\n   macro avg       0.70      0.70      0.68      4152\nweighted avg       0.72      0.73      0.71      4152\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/discriminant_analysis.py:926: UserWarning: Variables are collinear\n  warnings.warn(\"Variables are collinear\")\n","output_type":"stream"},{"name":"stdout","text":"Naive Bayes Best Accuracy: 0.6664\n              precision    recall  f1-score   support\n\n           0       0.69      0.84      0.76       510\n           1       0.66      0.46      0.54       579\n           2       0.40      0.65      0.49       588\n           3       0.73      0.94      0.82       649\n           4       0.95      0.99      0.97       830\n           5       0.59      0.27      0.37       491\n           6       0.53      0.25      0.34       505\n\n    accuracy                           0.67      4152\n   macro avg       0.65      0.63      0.61      4152\nweighted avg       0.67      0.67      0.65      4152\n\nSVM Best Accuracy: 0.8716\n              precision    recall  f1-score   support\n\n           0       0.88      0.95      0.91       510\n           1       0.87      0.82      0.84       579\n           2       0.85      0.84      0.84       588\n           3       0.94      0.96      0.95       649\n           4       1.00      0.99      1.00       830\n           5       0.74      0.71      0.72       491\n           6       0.73      0.73      0.73       505\n\n    accuracy                           0.87      4152\n   macro avg       0.86      0.86      0.86      4152\nweighted avg       0.87      0.87      0.87      4152\n\nSubmission file for Logistic_Regression saved.\nSubmission file for LDA saved.\nSubmission file for QDA saved.\nSubmission file for Naive_Bayes saved.\nSubmission file for SVM saved.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\n\n#This code just changes the names of the submission output to submit \n#Submission 1: Logistic Regression\n# Copy the file while keeping the original\n#shutil.copy('submission_Logistic_Regression.csv', 'submission.csv')\n\n#Submission 2: LDA\n# Copy the file while keeping the original\n#shutil.copy('submission_LDA.csv', 'submission.csv')\n\n#Submission 3: QDA\n# Copy the file while keeping the original\n#shutil.copy('submission_QDA.csv', 'submission.csv')\n\n#Submission 4: Naive Bayes\n# Copy the file while keeping the original\n#shutil.copy('submission_Naive_Bayes.csv', 'submission.csv')\n\n#Submission 5: SVM\n# Copy the file while keeping the original\nshutil.copy('submission_SVM.csv', 'submission.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T00:42:23.379926Z","iopub.execute_input":"2025-03-24T00:42:23.380358Z","iopub.status.idle":"2025-03-24T00:42:23.387568Z","shell.execute_reply.started":"2025-03-24T00:42:23.380324Z","shell.execute_reply":"2025-03-24T00:42:23.386682Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'submission.csv'"},"metadata":{}}],"execution_count":14}]}