{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29781,"databundleVersionId":2887556,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Store Sales Forecasting: ETS and ARIMA Models\n\n# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Load Data\ntrain = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\", parse_dates=['date'])\ntest = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\", parse_dates=['date'])\nstore_info = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/stores.csv\")\noil = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/oil.csv\", parse_dates=['date'])\nholidays = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\", parse_dates=['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T03:47:49.236508Z","iopub.execute_input":"2025-04-11T03:47:49.236792Z","iopub.status.idle":"2025-04-11T03:47:51.343437Z","shell.execute_reply.started":"2025-04-11T03:47:49.236770Z","shell.execute_reply":"2025-04-11T03:47:51.342606Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Focus on one store and product family: Store 1, \"GROCERY I\"\ndf = train[(train['store_nbr'] == 1) & (train['family'] == 'GROCERY I')].copy()\ndf = df[['date', 'sales']].set_index('date').asfreq('D').fillna(0)\n\n# Plot the sales data\nplt.figure(figsize=(12, 4))\nplt.plot(df.index, df['sales'], marker='o', linestyle='-', label='Sales')\nplt.title('Daily Sales - Store 1, GROCERY I')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:08:25.610343Z","iopub.execute_input":"2025-04-11T04:08:25.610753Z","iopub.status.idle":"2025-04-11T04:08:26.055306Z","shell.execute_reply.started":"2025-04-11T04:08:25.610723Z","shell.execute_reply":"2025-04-11T04:08:26.054460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## 2. Stationarity Check Using ADF Test\n#\n# The Augmented Dickey-Fuller (ADF) test helps us check whether our time series is stationary. A p-value larger than 0.05 suggests that the time series is non-stationary.\n\n# Perform ADF Test\nresult = adfuller(df['sales'])\nprint(\"ADF Statistic:\", result[0])\nprint(\"p-value:\", result[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:08:28.248681Z","iopub.execute_input":"2025-04-11T04:08:28.249348Z","iopub.status.idle":"2025-04-11T04:08:28.301533Z","shell.execute_reply.started":"2025-04-11T04:08:28.249319Z","shell.execute_reply":"2025-04-11T04:08:28.300576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## 3. ETS Model: Exponential Smoothing\n#\n# We use an additive trend and seasonal component (with a weekly seasonality of 7 days) to fit the ETS model.\n\n# Build the ETS Model\nets_model = ExponentialSmoothing(df['sales'], trend='add', seasonal='add', seasonal_periods=7).fit()\ndf['ETS_Forecast'] = ets_model.fittedvalues\n\n# Forecast the next 15 days\nforecast_ets = ets_model.forecast(15)\n\n# Plot ETS fit on training data\nplt.figure(figsize=(12, 4))\nplt.plot(df.index, df['sales'], label='Actual Sales')\nplt.plot(df.index, df['ETS_Forecast'], label='ETS Fitted', linestyle='--')\nplt.title('ETS Model Fit')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.legend()\nplt.show()\n\n# Plot ETS Forecast\nplt.figure(figsize=(10, 4))\nplt.plot(pd.date_range(df.index[-1] + pd.Timedelta(days=1), periods=15, freq='D'),\n         forecast_ets, marker='o', linestyle='-')\nplt.title('ETS Forecast - Next 15 Days')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:08:48.533692Z","iopub.execute_input":"2025-04-11T04:08:48.533983Z","iopub.status.idle":"2025-04-11T04:08:49.281253Z","shell.execute_reply.started":"2025-04-11T04:08:48.533960Z","shell.execute_reply":"2025-04-11T04:08:49.280233Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from statsmodels.graphics.tsaplots import plot_acf \nfrom statsmodels.graphics.tsaplots import plot_pacf \nplot_acf(df['sales']) \nplot_pacf(df['sales'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:08:10.928397Z","iopub.execute_input":"2025-04-11T04:08:10.928700Z","iopub.status.idle":"2025-04-11T04:08:11.285582Z","shell.execute_reply.started":"2025-04-11T04:08:10.928677Z","shell.execute_reply":"2025-04-11T04:08:11.284646Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## 4. ARIMA Model using SARIMAX\n# Here, we use:\n# - Non-seasonal order: (1, 1, 1)\n# - Seasonal order: (1, 1, 1, 7)\n\n# Build ARIMA Model using SARIMAX\narima_model = SARIMAX(df['sales'], order=(1,1,1), seasonal_order=(1,1,1,7)).fit(disp=False)\nprint(arima_model.summary())\n\n# Forecast the next 15 days\nforecast_arima = arima_model.get_forecast(steps=15).predicted_mean\n\n# Plot ARIMA Forecast\nplt.figure(figsize=(10, 4))\nplt.plot(df.index[-60:], df['sales'][-60:], label='Actual Sales')\nplt.plot(pd.date_range(df.index[-1] + pd.Timedelta(days=1), periods=15, freq='D'),\n         forecast_arima, marker='o', linestyle='-', label='ARIMA Forecast')\nplt.title('ARIMA Forecast - Next 15 Days')\nplt.xlabel('Date')\nplt.ylabel('Sales')\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:08:53.713204Z","iopub.execute_input":"2025-04-11T04:08:53.713521Z","iopub.status.idle":"2025-04-11T04:09:00.168978Z","shell.execute_reply.started":"2025-04-11T04:08:53.713499Z","shell.execute_reply":"2025-04-11T04:09:00.168006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# After fitting the ARIMA model (arima_model)\n# Extract the residuals from the ARIMA model\nresiduals = arima_model.resid\n\n# Plot the Residuals, ACF, and PACF for multiple lags\nimport statsmodels.api as sm\n\nplt.figure(figsize=(14, 8))\n\n# Residual Plot\nplt.subplot(311)\nplt.plot(residuals)\nplt.title('Residuals of the SARIMAX Model')\nplt.xlabel('Time')\nplt.ylabel('Residuals')\n\n# ACF Plot\nplt.subplot(312)\nsm.graphics.tsa.plot_acf(residuals, lags=30, ax=plt.gca())\nplt.title('ACF of Residuals')\n\n# PACF Plot\nplt.subplot(313)\nsm.graphics.tsa.plot_pacf(residuals, lags=30, ax=plt.gca(), method='ywm')\nplt.title('PACF of Residuals')\n\nplt.tight_layout()\nplt.show()\n\n# Ljung-Box test for a set of lags\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\n# Test for autocorrelation at multiple lags\nljung_box_results = acorr_ljungbox(residuals, lags=[1, 2, 3, 4, 5, 10, 15, 20], return_df=True)\nprint(\"Ljung-Box test results:\")\nprint(ljung_box_results)\n\n# Interpreting results:\n# A high p-value indicates that we fail to reject the null hypothesis of no autocorrelation.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:18:26.566276Z","iopub.execute_input":"2025-04-11T04:18:26.566704Z","iopub.status.idle":"2025-04-11T04:18:27.141320Z","shell.execute_reply.started":"2025-04-11T04:18:26.566681Z","shell.execute_reply":"2025-04-11T04:18:27.140517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ## 5. Preparing Submission Files\n# Step 1. Prepare the training data for store 1 and \"GROCERY I\"\ndf = train[(train['store_nbr'] == 1) & (train['family'] == 'GROCERY I')].copy()\ndf = df[['date', 'sales']].set_index('date').sort_index().asfreq('D').fillna(0)\n\n# Fit the ETS model (example configuration)\nets_model = ExponentialSmoothing(df['sales'], trend='add', seasonal='add', seasonal_periods=7).fit()\n\n# Forecast for the next 15 days (adjust as necessary)\nforecast_ets = ets_model.forecast(15)\n\n# Prepare a forecast DataFrame for the dates and predicted sales\nforecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=15, freq='D')\nforecast_df = pd.DataFrame({'date': forecast_dates, 'predicted_sales': forecast_ets.values})\nprint(\"Forecast DataFrame:\")\nprint(forecast_df)\n\n# Step 2. Prepare the test file\ntest['date'] = pd.to_datetime(test['date'])\n\n# Initialize a new column 'sales' with a default numeric value (0.0)\n# We set everything to 0.0 for combinations that are not store 1 and \"GROCERY I\"\ntest['sales'] = 0.0\n\n# Define a mask for rows corresponding to store 1 and \"GROCERY I\"\nmask = (test['store_nbr'] == 1) & (test['family'] == 'GROCERY I')\n\n# For the rows that match, map the forecast based on the date\n# We use the forecast_df's date as index\nforecast_series = forecast_df.set_index('date')['predicted_sales']\n\n# Use map on the 'date' column in the matching subset. This may produce NaNs if a date is not found.\n# Then fill any missing values with 0.0.\ntest.loc[mask, 'sales'] = test.loc[mask, 'date'].map(forecast_series).fillna(0.0)\n\n# Ensure the sales column is of float type\ntest['sales'] = test['sales'].astype(float)\n\n# Optionally, check for any empty or non-numeric values in the 'sales' column:\nif test['sales'].isnull().sum() > 0:\n    print(\"There are still missing values in the sales column; filling with 0.0\")\n    test['sales'] = test['sales'].fillna(0.0)\n\n# Step 3. Create the submission DataFrame and file\nsubmission = test[['id', 'sales']].copy()\n\n# Ensure no empty strings are present (they should be numeric 0.0 if missing)\nsubmission['sales'] = submission['sales'].apply(lambda x: float(x) if pd.notnull(x) else 0.0)\n\n#submission.to_csv('submission.csv', index=False)\nprint(\"Submission file saved as submission.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:35:13.983261Z","iopub.execute_input":"2025-04-11T04:35:13.983569Z","iopub.status.idle":"2025-04-11T04:35:14.610253Z","shell.execute_reply.started":"2025-04-11T04:35:13.983541Z","shell.execute_reply":"2025-04-11T04:35:14.609111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#ARIMA sub\n# Step 1. Filter training data for store 1 and GROCERY I\ndf = train[(train['store_nbr'] == 1) & (train['family'] == 'GROCERY I')].copy()\ndf = df[['date', 'sales']].set_index('date').sort_index().asfreq('D').fillna(0)\n\n# Step 2. Fit SARIMAX model (use best config based on earlier analysis)\nmodel = SARIMAX(df['sales'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 7), enforce_stationarity=False, enforce_invertibility=False)\nresults = model.fit(disp=False)\n\n# Step 3. Forecast next 15 days\nforecast_arima = results.forecast(steps=15)\n\n# Create forecast DataFrame\nforecast_dates = pd.date_range(start=df.index[-1] + pd.Timedelta(days=1), periods=15, freq='D')\nforecast_df = pd.DataFrame({'date': forecast_dates, 'predicted_sales': forecast_arima.values})\n\n# Step 4. Prepare test set\ntest['date'] = pd.to_datetime(test['date'])\ntest['sales'] = 0.0  # Initialize all with default\n\n# Define mask for store 1 & GROCERY I\nmask = (test['store_nbr'] == 1) & (test['family'] == 'GROCERY I')\n\n# Map forecast values to matching test rows\nforecast_series = forecast_df.set_index('date')['predicted_sales']\ntest.loc[mask, 'sales'] = test.loc[mask, 'date'].map(forecast_series).fillna(0.0)\n\n# Ensure numeric type\ntest['sales'] = test['sales'].astype(float)\n\n# Step 5. Write submission\nsubmission = test[['id', 'sales']].copy()\nsubmission['sales'] = submission['sales'].apply(lambda x: float(x) if pd.notnull(x) else 0.0)\nsubmission.to_csv('submission.csv', index=False)\nprint(\"ARIMA submission saved as submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T04:42:30.734885Z","iopub.execute_input":"2025-04-11T04:42:30.735212Z","iopub.status.idle":"2025-04-11T04:42:37.325825Z","shell.execute_reply.started":"2025-04-11T04:42:30.735188Z","shell.execute_reply":"2025-04-11T04:42:37.324987Z"}},"outputs":[],"execution_count":null}]}