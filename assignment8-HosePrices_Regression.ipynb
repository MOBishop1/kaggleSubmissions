{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 0. Package load\n# -----------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom sklearn.linear_model import LinearRegression, LassoCV\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:43.704324Z","iopub.execute_input":"2025-04-13T05:10:43.704591Z","iopub.status.idle":"2025-04-13T05:10:43.709539Z","shell.execute_reply.started":"2025-04-13T05:10:43.704573Z","shell.execute_reply":"2025-04-13T05:10:43.708801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 1. Load Data and Display Histograms\n# -----------------------\n# Load data files (ensure these are in your working directory)\ntrain = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\ntest_ids = test[\"Id\"]\n\n# Drop Id from both sets for modeling\ntrain.drop(\"Id\", axis=1, inplace=True)\ntest.drop(\"Id\", axis=1, inplace=True)\n\n# Plot histogram of the original SalePrice distribution (raw, non-transformed)\nplt.figure(figsize=(8,5))\nplt.hist(train[\"SalePrice\"], bins=40, color='lightgreen', edgecolor='black')\nplt.title(\"Original SalePrice Distribution\")\nplt.xlabel(\"SalePrice\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n# Log-transform SalePrice (to help reduce right-tail skew) and plot its histogram\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nplt.figure(figsize=(8,5))\nplt.hist(train[\"SalePrice\"], bins=40, color='skyblue', edgecolor='black')\nplt.title(\"Log-Transformed SalePrice Distribution\")\nplt.xlabel(\"log(SalePrice)\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:46.414522Z","iopub.execute_input":"2025-04-13T05:10:46.414794Z","iopub.status.idle":"2025-04-13T05:10:46.756631Z","shell.execute_reply.started":"2025-04-13T05:10:46.414775Z","shell.execute_reply":"2025-04-13T05:10:46.755883Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 2. Preprocessing Setup\n# -----------------------\n# Separate target and features from the training set.\ny = train[\"SalePrice\"]\ntrain_features = train.drop(\"SalePrice\", axis=1)\n\n# Combine training features and test set for consistent preprocessing\nall_data = pd.concat([train_features, test], axis=0)\n\n# Identify numerical and categorical columns\nnumeric_cols = all_data.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncategorical_cols = all_data.select_dtypes(include=[\"object\"]).columns.tolist()\n\n# Create pipeline for numerical features: impute with median then scale\nnumeric_transformer = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"scaler\", StandardScaler())\n])\n\n# Create pipeline for categorical features: impute with most frequent then one-hot encode (drop first to avoid collinearity)\ncategorical_transformer = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"first\", sparse=False))\n])\n\n# Combine transformations in a ColumnTransformer\npreprocessor = ColumnTransformer([\n    (\"num\", numeric_transformer, numeric_cols),\n    (\"cat\", categorical_transformer, categorical_cols)\n])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:20:10.259422Z","iopub.execute_input":"2025-04-13T08:20:10.259652Z","iopub.status.idle":"2025-04-13T08:20:10.273971Z","shell.execute_reply.started":"2025-04-13T08:20:10.259637Z","shell.execute_reply":"2025-04-13T08:20:10.273223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 3. Model Pipelines\n# -----------------------\n# Model 1: Lasso Regression with Variable Selection after Polynomial (interaction) expansion.\n# We expose the PolynomialFeatures step as \"poly\" to retrieve feature names later.\nmodel1_pipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),  # includes interaction and squared terms\n    (\"lasso\", LassoCV(cv=5, random_state=1234, max_iter=10000))\n])\n\n# Model 2: PCA for dimension reduction after polynomial expansion, followed by Linear Regression.\nmodel2_pipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n    (\"pca\", PCA(n_components=100, random_state=1234)),  \n    (\"linreg\", LinearRegression())\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:20:17.474685Z","iopub.execute_input":"2025-04-13T08:20:17.475472Z","iopub.status.idle":"2025-04-13T08:20:17.480430Z","shell.execute_reply.started":"2025-04-13T08:20:17.475448Z","shell.execute_reply":"2025-04-13T08:20:17.479299Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 4. Split Data for Evaluation\n# -----------------------\n# Split only the training data (the rows in all_data corresponding to train_features)\nX_all = all_data.iloc[:train_features.shape[0], :]\nX_train, X_val, y_train, y_val = train_test_split(X_all, y, test_size=0.2, random_state=1234)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:13:58.819433Z","iopub.execute_input":"2025-04-13T05:13:58.819673Z","iopub.status.idle":"2025-04-13T05:13:58.828744Z","shell.execute_reply.started":"2025-04-13T05:13:58.819657Z","shell.execute_reply":"2025-04-13T05:13:58.828136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 5. Fit and Evaluate Model 1 (Lasso with Polynomial Features)\n# -----------------------\n# Fit Model 1 on training set\nmodel1_pipeline.fit(X_train, y_train)\n\n# Predict on validation set and evaluate using RMSE (convert predictions back from log-scale with np.expm1)\ny_pred_val_m1 = model1_pipeline.predict(X_val)\nrmse_model1 = np.sqrt(mean_squared_error(np.expm1(y_val), np.expm1(y_pred_val_m1)))\nprint(f\"Model 1 (Lasso) RMSE on validation set: {rmse_model1:.4f}\")\n\n# --- Assumption Checks for Model 1 ---\n# Residuals and Q-Q Plot (on log-transformed scale)\ny_pred_train_m1 = model1_pipeline.predict(X_train)\nresiduals = y_train - y_pred_train_m1\n\nplt.figure(figsize=(8,5))\nplt.hist(residuals, bins=40, color='violet', edgecolor='black')\nplt.title(\"Model 1 Residuals Histogram\")\nplt.xlabel(\"Residual\")\nplt.ylabel(\"Frequency\")\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(8,5))\nstats.probplot(residuals, dist=\"norm\", plot=plt)\nplt.title(\"Model 1 Q-Q Plot of Residuals\")\nplt.show()\n\n# Print cross-validated RMSE as well\ncv_rmse_model1 = np.mean(np.sqrt(-cross_val_score(model1_pipeline, X_all, y, scoring=\"neg_mean_squared_error\", cv=5)))\n#print(f\"Model 1 Cross-Validated RMSE: {cv_rmse_model1:.4f}\")\n\n# --- Model 1 Coefficients (Non-zero) ---\n# Retrieve the names of features after preprocessing and polynomial expansion.\npreproc_feature_names = model1_pipeline.named_steps[\"preprocessor\"].get_feature_names_out()\npoly_feature_names = model1_pipeline.named_steps[\"poly\"].get_feature_names_out(preproc_feature_names)\n\nlasso_model = model1_pipeline.named_steps[\"lasso\"]\nnonzero_indices = np.where(lasso_model.coef_ != 0)[0]\nprint(\"\\n--- Nonzero Coefficients from Model 1 (Lasso) ---\")\nfor idx in nonzero_indices:\n    print(f\"{poly_feature_names[idx]}: {lasso_model.coef_[idx]:.4f}\")\n    print(f\"Intercept: {lasso_model.intercept_:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:14:59.644800Z","iopub.execute_input":"2025-04-13T05:14:59.645019Z","iopub.status.idle":"2025-04-13T07:43:49.070960Z","shell.execute_reply.started":"2025-04-13T05:14:59.645004Z","shell.execute_reply":"2025-04-13T07:43:49.069771Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 6. Fit and Evaluate Model 2 (PCA + Linear Regression)\n# -----------------------\n#model2_pipeline.fit(X_all, y)  # fit on all training data\n#y_pred_val_m2 = model2_pipeline.predict(X_val)\n#rmse_model2 = np.sqrt(mean_squared_error(np.expm1(y_val), np.expm1(y_pred_val_m2)))\n#print(f\"\\nModel 2 (PCA + Linear Regression) RMSE on validation set: {rmse_model2:.4f}\")\n\n# Print PCA summary details: number of components and some regression coefficients on PCA components.\n#pca_model = model2_pipeline.named_steps[\"pca\"]\n#print(f\"Model 2 - Number of PCA Components retained: {pca_model.n_components_}\")\n#linreg_model = model2_pipeline.named_steps[\"linreg\"]\n#print(\"Model 2 - Linear Regression Coefficients on PCA components:\")\n#print(linreg_model.coef_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:20:27.534524Z","iopub.execute_input":"2025-04-13T08:20:27.534813Z","iopub.status.idle":"2025-04-13T08:20:33.967601Z","shell.execute_reply.started":"2025-04-13T08:20:27.534792Z","shell.execute_reply":"2025-04-13T08:20:33.966974Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -----------------------\n# 7. Final Predictions and Kaggle Submission Files\n# -----------------------\n# Prepare the test set (rows in all_data corresponding to test features).\nX_test_final = all_data.iloc[train_features.shape[0]:, :]\n\n# Model 1 predictions on the test set and export the submission file.\nmodel1_test_preds = model1_pipeline.predict(X_test_final)\nsubmission_model1 = pd.DataFrame({\n    \"Id\": test_ids,\n    \"SalePrice\": np.expm1(model1_test_preds)\n})\nsubmission_model1.to_csv(\"submission.csv\", index=False)\nprint(\"\\nKaggle submission file 'submission_model1.csv' created for Model 1.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:28:41.373606Z","iopub.execute_input":"2025-04-13T08:28:41.373890Z","iopub.status.idle":"2025-04-13T08:28:41.378954Z","shell.execute_reply.started":"2025-04-13T08:28:41.373868Z","shell.execute_reply":"2025-04-13T08:28:41.377655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model 2 predictions on the test set and export the submission file.\n#model2_test_preds = model2_pipeline.predict(X_test_final)\n#submission_model2 = pd.DataFrame({\n#    \"Id\": test_ids,\n#    \"SalePrice\": np.expm1(model2_test_preds)\n#})\n#submission_model2.to_csv(\"submission.csv\", index=False)\n#print(\"Kaggle submission file 'submission_model2.csv' created for Model 2.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T08:29:13.900382Z","iopub.execute_input":"2025-04-13T08:29:13.900629Z","iopub.status.idle":"2025-04-13T08:29:14.379471Z","shell.execute_reply.started":"2025-04-13T08:29:13.900602Z","shell.execute_reply":"2025-04-13T08:29:14.378553Z"}},"outputs":[],"execution_count":null}]}